{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Pre-train + Fine-tune + Prompting (Bonus 20 points)\n",
        "\n",
        "In this notebook, we will:\n",
        "1. **Part 4A (12 points)**: Pre-train a Transformer LM on TinyStories, generate samples with greedy and top-k decoding, and fine-tune on Multiple-choice QA\n",
        "2. **Part 4B (8 points)**: Experiment with prompting strategies\n",
        "\n",
        "## Deliverables:\n",
        "- `Part4.ipynb` - This notebook with the complete pipeline\n",
        "- `Trained_transformer_predictions.npy` - Predicted QA labels from fine-tuned model\n",
        "- `Trained_transformer_predictions_scalings.npy` - Predicted QA labels from prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Add directories to path for imports\n",
        "# Note: part3 must be in path for nn_utils which is used by part2/model.py\n",
        "sys.path.insert(0, '.')\n",
        "sys.path.insert(0, 'part1')\n",
        "sys.path.insert(0, 'part2')\n",
        "sys.path.insert(0, 'part3')  # Required for nn_utils\n",
        "sys.path.insert(0, 'part4')\n",
        "\n",
        "# Import from Part 1: Tokenizer\n",
        "from part1.tokenizer import Tokenizer, get_tokenizer\n",
        "from part1.train_bpe import train_bpe\n",
        "\n",
        "# Import from Part 2: Transformer Model\n",
        "from part2.model import TransformerLM, count_parameters\n",
        "\n",
        "# Import from Part 3: Training utilities\n",
        "from part3.nn_utils import cross_entropy, gradient_clipping\n",
        "\n",
        "# Import from Part 4: Pre-training, Fine-tuning, Prompting\n",
        "from part4.sampling import greedy_decode, top_k_decode, generate_text\n",
        "from part4.datasets import PretrainingDataset, MultipleChoiceQADataset, create_pretraining_dataloader, create_qa_dataloader\n",
        "from part4.trainer import Trainer, TrainingConfig, create_qa_loss_fn\n",
        "from part4.qa_model import TransformerForMultipleChoice, evaluate_qa_model\n",
        "from part4.prompting import PromptTemplate, PromptingPipeline, evaluate_prompting\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4A: Pre-training + Fine-tuning (12 points)\n",
        "\n",
        "## Step 1: Setup Tokenizer\n",
        "\n",
        "We'll use either a pre-trained GPT-2 tokenizer or train our own BPE tokenizer on TinyStories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Load GPT-2 tokenizer (recommended for better results)\n",
        "def load_gpt2_tokenizer():\n",
        "    \"\"\"Load pre-trained GPT-2 tokenizer from fixtures.\"\"\"\n",
        "    import json\n",
        "    \n",
        "    vocab_path = Path(\"part1/fixtures/gpt2_vocab.json\")\n",
        "    merges_path = Path(\"part1/fixtures/gpt2_merges.txt\")\n",
        "    \n",
        "    # Load vocab\n",
        "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        vocab_str = json.load(f)\n",
        "    \n",
        "    # Convert vocab keys from strings to bytes\n",
        "    vocab = {}\n",
        "    for idx, (token_str, token_id) in enumerate(vocab_str.items()):\n",
        "        # GPT-2 uses unicode escapes, need to convert carefully\n",
        "        vocab[token_id] = token_str.encode(\"utf-8\") if isinstance(token_str, str) else token_str\n",
        "    \n",
        "    # Load merges\n",
        "    merges = []\n",
        "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line and not line.startswith(\"#\"):\n",
        "                parts = line.split()\n",
        "                if len(parts) == 2:\n",
        "                    merges.append((parts[0].encode(\"utf-8\"), parts[1].encode(\"utf-8\")))\n",
        "    \n",
        "    return get_tokenizer(vocab, merges, special_tokens=[\"<|endoftext|>\"])\n",
        "\n",
        "# Option 2: Train BPE tokenizer on TinyStories (smaller vocab, faster)\n",
        "def train_tiny_tokenizer(vocab_size=1000):\n",
        "    \"\"\"Train a small BPE tokenizer on TinyStories.\"\"\"\n",
        "    input_path = Path(\"part1/fixtures/tinystories_sample.txt\")\n",
        "    vocab, merges = train_bpe(input_path, vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])\n",
        "    return get_tokenizer(vocab, merges, special_tokens=[\"<|endoftext|>\"])\n",
        "\n",
        "# Use Option 2 for this assignment (smaller, faster training)\n",
        "print(\"Training BPE tokenizer on TinyStories...\")\n",
        "tokenizer = train_tiny_tokenizer(vocab_size=1000)\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Get special token ID\n",
        "eos_token_id = tokenizer.encode(\"<|endoftext|>\")[0] if \"<|endoftext|>\" in tokenizer.special_tokens else None\n",
        "print(f\"EOS token ID: {eos_token_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Transformer LM\n",
        "\n",
        "We create a small transformer model suitable for the TinyStories dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model hyperparameters (small model for quick training)\n",
        "model_config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"context_length\": 256,\n",
        "    \"d_model\": 128,\n",
        "    \"num_layers\": 4,\n",
        "    \"num_heads\": 4,\n",
        "    \"d_ff\": 512,\n",
        "    \"rope_theta\": 10000.0,\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerLM(**model_config)\n",
        "model = model.to(device)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(f\"Model parameters: {num_params:,}\")\n",
        "print(f\"Model config: {model_config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Pre-train on TinyStories\n",
        "\n",
        "Pre-train the transformer on the TinyStories dataset using next-token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pre-training dataloader\n",
        "pretrain_dataloader = create_pretraining_dataloader(\n",
        "    file_path=\"part1/fixtures/tinystories_sample.txt\",\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=8,\n",
        "    max_length=256,\n",
        "    stride=128,  # Overlapping sequences\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "print(f\"Number of pre-training batches: {len(pretrain_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-training configuration\n",
        "pretrain_config = TrainingConfig(\n",
        "    num_epochs=3,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=50,\n",
        "    max_grad_norm=1.0,\n",
        "    log_interval=10,\n",
        "    device=device,\n",
        "    checkpoint_dir=\"checkpoints/pretrain\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "pretrain_trainer = Trainer(\n",
        "    model=model,\n",
        "    config=pretrain_config,\n",
        "    train_dataloader=pretrain_dataloader,\n",
        ")\n",
        "\n",
        "# Run pre-training\n",
        "print(\"Starting pre-training...\")\n",
        "pretrain_history = pretrain_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Samples with Greedy and Top-k Decoding\n",
        "\n",
        "Test the pre-trained model by generating text samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Once upon a time\",\n",
        "    \"The little girl\",\n",
        "    \"One day, a\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GREEDY DECODING\")\n",
        "print(\"=\" * 60)\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=50,\n",
        "        method=\"greedy\",\n",
        "        eos_token_id=eos_token_id,\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TOP-K DECODING (k=50, temperature=0.8)\")\n",
        "print(\"=\" * 60)\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=50,\n",
        "        method=\"top_k\",\n",
        "        k=50,\n",
        "        temperature=0.8,\n",
        "        eos_token_id=eos_token_id,\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine-tune on Multiple-Choice QA\n",
        "\n",
        "Fine-tune the pre-trained model on a multiple-choice QA dataset. The model predicts the correct answer using pooled representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load QA datasets\n",
        "with open(\"part4/fixtures/qa_train.json\", \"r\") as f:\n",
        "    qa_train_data = json.load(f)\n",
        "\n",
        "with open(\"part4/fixtures/qa_dev.json\", \"r\") as f:\n",
        "    qa_dev_data = json.load(f)\n",
        "\n",
        "with open(\"part4/fixtures/qa_test.json\", \"r\") as f:\n",
        "    qa_test_data = json.load(f)\n",
        "\n",
        "print(f\"Training examples: {len(qa_train_data)}\")\n",
        "print(f\"Dev examples: {len(qa_dev_data)}\")\n",
        "print(f\"Test examples: {len(qa_test_data)}\")\n",
        "\n",
        "# Create dataloaders\n",
        "qa_train_loader = create_qa_dataloader(\n",
        "    data=qa_train_data,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=4,\n",
        "    max_length=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "qa_dev_loader = create_qa_dataloader(\n",
        "    data=qa_dev_data,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=4,\n",
        "    max_length=128,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "qa_test_loader = create_qa_dataloader(\n",
        "    data=qa_test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=4,\n",
        "    max_length=128,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create QA model with pre-trained backbone\n",
        "qa_model = TransformerForMultipleChoice(\n",
        "    transformer_lm=model,\n",
        "    hidden_size=model_config[\"d_model\"],\n",
        "    num_choices=4,\n",
        "    pooling=\"last\",  # Use last token representation\n",
        "    freeze_backbone=False,  # Fine-tune the whole model\n",
        ")\n",
        "qa_model = qa_model.to(device)\n",
        "\n",
        "print(f\"QA Model parameters: {count_parameters(qa_model):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning configuration\n",
        "finetune_config = TrainingConfig(\n",
        "    num_epochs=5,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=20,\n",
        "    max_grad_norm=1.0,\n",
        "    log_interval=5,\n",
        "    device=device,\n",
        "    checkpoint_dir=\"checkpoints/finetune\",\n",
        "    patience=3,  # Early stopping\n",
        ")\n",
        "\n",
        "# Create trainer with QA loss function\n",
        "finetune_trainer = Trainer(\n",
        "    model=qa_model,\n",
        "    config=finetune_config,\n",
        "    train_dataloader=qa_train_loader,\n",
        "    val_dataloader=qa_dev_loader,\n",
        "    compute_loss_fn=create_qa_loss_fn(device),\n",
        ")\n",
        "\n",
        "# Run fine-tuning\n",
        "print(\"Starting fine-tuning on QA...\")\n",
        "finetune_history = finetune_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on dev set\n",
        "dev_results = evaluate_qa_model(qa_model, qa_dev_loader, device)\n",
        "print(f\"\\nDev Accuracy: {dev_results['accuracy']:.4f}\")\n",
        "\n",
        "# Generate predictions on test set\n",
        "test_results = evaluate_qa_model(qa_model, qa_test_loader, device)\n",
        "predictions_finetune = np.array(test_results['predictions'])\n",
        "\n",
        "print(f\"Test predictions shape: {predictions_finetune.shape}\")\n",
        "print(f\"Test predictions: {predictions_finetune}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions from fine-tuned model\n",
        "np.save(\"Trained_transformer_predictions.npy\", predictions_finetune)\n",
        "print(\"Saved fine-tuned model predictions to Trained_transformer_predictions.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4B: Prompting (8 points)\n",
        "\n",
        "Experiment with different prompting strategies for multiple-choice QA.\n",
        "\n",
        "## Step 1: Zero-Shot Prompting\n",
        "\n",
        "Use the language model to directly predict answers based on prompt formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create prompting pipeline with basic template\n",
        "basic_template = PromptTemplate(template_name=\"basic\")\n",
        "prompting_pipeline = PromptingPipeline(\n",
        "    model=model,  # Use the pre-trained/fine-tuned LM\n",
        "    tokenizer=tokenizer,\n",
        "    template=basic_template,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Test on a single example\n",
        "example = qa_dev_data[0]\n",
        "print(\"Example:\")\n",
        "print(f\"  Context: {example['context'][:100]}...\")\n",
        "print(f\"  Question: {example['question']}\")\n",
        "print(f\"  Choices: {example['choices']}\")\n",
        "print(f\"  True answer: {example['answer']}\")\n",
        "\n",
        "pred, probs = prompting_pipeline.predict_single(\n",
        "    example[\"context\"],\n",
        "    example[\"question\"],\n",
        "    example[\"choices\"],\n",
        "    return_probs=True,\n",
        ")\n",
        "print(f\"  Predicted: {pred}\")\n",
        "print(f\"  Probabilities: {probs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate zero-shot prompting on dev set\n",
        "print(\"Evaluating zero-shot prompting on dev set...\")\n",
        "zeroshot_results = evaluate_prompting(prompting_pipeline, qa_dev_data)\n",
        "print(f\"Zero-shot Dev Accuracy: {zeroshot_results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Different Prompt Templates\n",
        "\n",
        "Experiment with different prompt styles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different templates\n",
        "template_names = [\"basic\", \"instruction\", \"simple\"]\n",
        "\n",
        "for template_name in template_names:\n",
        "    template = PromptTemplate(template_name=template_name)\n",
        "    pipeline = PromptingPipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        template=template,\n",
        "        device=device,\n",
        "    )\n",
        "    \n",
        "    results = evaluate_prompting(pipeline, qa_dev_data)\n",
        "    print(f\"Template '{template_name}': Dev Accuracy = {results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Few-Shot Prompting\n",
        "\n",
        "Use example demonstrations to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select few-shot examples from training data\n",
        "few_shot_examples = qa_train_data[:3]  # Use 3 examples\n",
        "\n",
        "print(\"Few-shot examples:\")\n",
        "for i, ex in enumerate(few_shot_examples):\n",
        "    print(f\"  {i+1}. Q: {ex['question'][:50]}... A: {ex['choices'][ex['answer']]}\")\n",
        "\n",
        "# Test few-shot on a dev example\n",
        "test_example = qa_dev_data[0]\n",
        "pred = prompting_pipeline.few_shot_predict(\n",
        "    test_example[\"context\"],\n",
        "    test_example[\"question\"],\n",
        "    test_example[\"choices\"],\n",
        "    few_shot_examples=few_shot_examples,\n",
        ")\n",
        "print(f\"\\nFew-shot prediction: {pred}\")\n",
        "print(f\"True answer: {test_example['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate few-shot prompting on dev set\n",
        "print(\"Evaluating few-shot prompting on dev set...\")\n",
        "fewshot_predictions = []\n",
        "for example in qa_dev_data:\n",
        "    pred = prompting_pipeline.few_shot_predict(\n",
        "        example[\"context\"],\n",
        "        example[\"question\"],\n",
        "        example[\"choices\"],\n",
        "        few_shot_examples=few_shot_examples,\n",
        "    )\n",
        "    fewshot_predictions.append(pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = sum(1 for pred, ex in zip(fewshot_predictions, qa_dev_data) if pred == ex[\"answer\"])\n",
        "accuracy = correct / len(qa_dev_data)\n",
        "print(f\"Few-shot Dev Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Custom Prompt Engineering\n",
        "\n",
        "Design and test your own prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom prompt template\n",
        "custom_template = \"\"\"Story: {context}\n",
        "\n",
        "Based on the story above, answer this question:\n",
        "{question}\n",
        "\n",
        "Options:\n",
        "{choices_formatted}\n",
        "\n",
        "The correct answer is option\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    custom_template=custom_template,\n",
        "    choice_format=\"letter\",\n",
        ")\n",
        "\n",
        "custom_pipeline = PromptingPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    template=custom_prompt,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Evaluate custom prompt\n",
        "custom_results = evaluate_prompting(custom_pipeline, qa_dev_data)\n",
        "print(f\"Custom template Dev Accuracy: {custom_results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Test Predictions with Best Prompting Strategy\n",
        "\n",
        "Use the best-performing prompting strategy to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test predictions using the best prompting strategy\n",
        "# (Choose the one that performed best on dev set)\n",
        "\n",
        "print(\"Generating test predictions with prompting...\")\n",
        "\n",
        "# Use zero-shot with basic template (or change based on your best results)\n",
        "test_predictions_prompting = prompting_pipeline.predict_batch(qa_test_data)\n",
        "predictions_prompting = np.array(test_predictions_prompting)\n",
        "\n",
        "print(f\"Test predictions shape: {predictions_prompting.shape}\")\n",
        "print(f\"Test predictions: {predictions_prompting}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save prompting predictions\n",
        "np.save(\"Trained_transformer_predictions_scalings.npy\", predictions_prompting)\n",
        "print(\"Saved prompting predictions to Trained_transformer_predictions_scalings.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary and Results\n",
        "\n",
        "## Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PART 4 RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n--- Part 4A: Pre-training + Fine-tuning ---\")\n",
        "print(f\"Model parameters: {count_parameters(qa_model):,}\")\n",
        "print(f\"Pre-training epochs: {pretrain_config.num_epochs}\")\n",
        "print(f\"Fine-tuning epochs: {finetune_config.num_epochs}\")\n",
        "print(f\"Fine-tuned model Dev Accuracy: {dev_results['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n--- Part 4B: Prompting ---\")\n",
        "print(f\"Zero-shot Dev Accuracy: {zeroshot_results['accuracy']:.4f}\")\n",
        "print(f\"Custom template Dev Accuracy: {custom_results['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n--- Saved Files ---\")\n",
        "print(\"1. Trained_transformer_predictions.npy (fine-tuned model predictions)\")\n",
        "print(\"2. Trained_transformer_predictions_scalings.npy (prompting predictions)\")\n",
        "\n",
        "print(\"\\n--- Prediction Comparison ---\")\n",
        "print(f\"Fine-tuned predictions: {predictions_finetune}\")\n",
        "print(f\"Prompting predictions:  {predictions_prompting}\")\n",
        "agreement = np.mean(predictions_finetune == predictions_prompting)\n",
        "print(f\"Agreement between methods: {agreement:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverables Checklist\n",
        "\n",
        "- [x] `Part4.ipynb` - Complete pre-training, fine-tuning, and prompting pipeline\n",
        "- [x] `Trained_transformer_predictions.npy` - Predictions from fine-tuned model\n",
        "- [x] `Trained_transformer_predictions_scalings.npy` - Predictions from prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify output files exist\n",
        "import os\n",
        "\n",
        "files_to_check = [\n",
        "    \"Trained_transformer_predictions.npy\",\n",
        "    \"Trained_transformer_predictions_scalings.npy\",\n",
        "]\n",
        "\n",
        "print(\"Verifying output files:\")\n",
        "for f in files_to_check:\n",
        "    if os.path.exists(f):\n",
        "        arr = np.load(f)\n",
        "        print(f\"  ✓ {f} (shape: {arr.shape})\")\n",
        "    else:\n",
        "        print(f\"  ✗ {f} (not found)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
